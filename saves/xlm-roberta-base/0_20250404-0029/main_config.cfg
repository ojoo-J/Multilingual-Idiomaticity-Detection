[args]
bert_model=xlm-roberta-base
data_dir=data/preproc
task_name=idiom
classifier_hidden=768
lr_schedule=warmup_linear
warmup_epoch=2
drop_ratio=0.2
kfold=10
num_bagging=0
bagging_index=0
use_pos=False
use_local_context=False
max_seq_length=300
do_train=True
do_dev=True
do_eval=True
do_lower_case=False
class_weight=1
train_batch_size=16
eval_batch_size=8
learning_rate=3e-05
num_train_epoch=10
no_cuda=False
seed=42

