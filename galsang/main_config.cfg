[args]
# Bert pre-trained model selected in the list [bert-base-cased, roberta-base, albert-base-v1 / albert-large-v1] (default = roberta-base)
bert_model = xlm-roberta-base
# bert_model = xlm-roberta-large

# The input data dir. Should contain the .tsv files (VUA18 / VUAverb / MOH-X/CLS / TroFi/CLS / VUA20)
data_dir = data/preproc
# The name of the task to train (vua(1-fold) / trofi(10-fold))
task_name = idiom
# The name of model type (default = MELBERT) (BERT_BASE / BERT_SEQ / MELBERT_SPV / MELBERT_MIP / MELBERT)
# model_type = MELBERT_SPV
model_type = MELBERT_SPV_CONTEXT
# The hidden dimension for classifier (default = 768)
classifier_hidden = 768
# Learning rate scheduler (default = warmup_linear) (none / warmup_linear)
lr_schedule = warmup_linear
# Training epochs to perform linear learning rate warmup for. (default = 2)
warmup_epoch = 2
# Dropout ratio (default = ÃŸ0.2)
drop_ratio = 0.2

# The maximum total input sequence length after WordPiece tokenization. (default = 200)
max_seq_length = 300
# Whether to run training (default = False)
do_train = True
# Whether to run eval on the dev set. (default = False)
do_dev = True
# Whether to run eval on the test set (default = False)
do_eval = True
# Set this flag if you are using an uncased model. (default = False)
do_lower_case = False
# Weight of metaphor. (default = 3.0) ?
class_weight = 1
# Total batch size for training. (default = 32) !
train_batch_size = 32
# Total batch size for eval. (default = 8)
eval_batch_size = 8
# The initial learning rate for Adam (default = 3e-5) !
learning_rate = 3e-5
# Total number of training epochs to perform. (default = 3.0) !
num_train_epoch = 10

# Whether not to use CUDA when available (default = False)
no_cuda = False
# random seed for initialization (default = 42) !
seed = 42